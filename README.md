# Reward Models Inherit Value Biases from Pretraining

This repository contains code and data for analyzing how reward models trained on different base models (Gemma vs. Llama) inherit value-related biases from their pretrained representations. We evaluate this using two established psychological instruments — the Big Two personality framework (Agency/Communion) and the Moral Foundations Dictionary 2.0 — by examining how 10 off-the-shelf reward models from RewardBench rank value-laden tokens, by tracing these these biases back to their pretrained base-models and the "implicit" RMs between them, and by tracking how these biases emerge during reward model training.

## Repository Structure

```
config/
├── models.yaml                12 models used across Figures 2 and 3
└── prompts.yaml               54 prompts (6 adjectives × 3 superlatives × 3 concision styles)

data/
├── corpora/                    Value dictionaries and token lists
│   ├── dict_big2_expanded.csv          Big Two dictionary (expanded)
│   ├── dict_big2_nouns.csv             Big Two dictionary (nouns only)
│   ├── moral-foundations-dictionary-20.dicx   MFD 2.0
│   └── token_intersection.csv          Shared tokens across model vocabularies
├── reward_model_scores/        Reward scores from 10 off-the-shelf models
│   ├── dict_big2_nouns/                Scores on Big Two nouns
│   └── dict_MFD_20/                    Scores on MFD 2.0 terms
├── logprobs/                   Per-model log-probabilities (12 models × 54 prompts)
└── reward_model_training/      Checkpoint-level data from in-house training runs
    ├── greatest_ever_one_across_checkpoints_gemma.parquet
    ├── greatest_ever_one_across_checkpoints_llama.parquet
    └── rank_results_*.csv              Ablation results at various data sizes

scripts/
└── generate_logprobs.py        Generate per-model logprob CSVs (requires GPU)

figures/
├── generate_figure_1_and_A1.R         Off-the-shelf RM comparison (Big Two & MFD)
├── generate_figure_3.py        MWLR lollipop plot (Freedom vs Love)
├── generate_figure_4.R         Training checkpoint & ablation analysis
├── logprob_model_comparison.py Base-model log-probability comparison
└── output/                     Generated PDFs and PNGs
```

## Requirements

### R

Used by `generate_figure_1_and_A1.R` and `generate_figure_4.R`.

```r
install.packages(c(
  "tidyverse", "tidytext", "ggpubr", "ggbeeswarm",
  "extrafont", "nlme", "MKinfer",
  "jsonlite", "fuzzyjoin", "arrow"
))
```

### Python

Used by `scripts/generate_logprobs.py` (requires CUDA GPU), `figures/logprob_model_comparison.py`, and `figures/generate_figure_3.py`.

```
numpy
pandas
torch
transformers
huggingface_hub
seaborn
matplotlib
statsmodels
pyyaml
tqdm
```

## Reproducing Figures

All scripts assume the working directory is the repository root.

### Generating log-probabilities (prerequisite for Figures 2 and 3)

```bash
python scripts/generate_logprobs.py                     # all 12 models
python scripts/generate_logprobs.py --model google/gemma-2-2b  # one model
```

Requires a CUDA-capable GPU and HuggingFace access tokens for gated models. Produces per-model CSVs in `data/logprobs/` (one file per model, 54 prompt columns each). Skips models whose output file already exists.

### Figure 1 — Off-the-shelf reward model rankings (Big Two & MFD)

```bash
Rscript figures/generate_figure_1_and_A1.R
```

Produces in `figures/output/`:
- `fig1a_big2.pdf` — Figure 1(a) (Big Two, mean + SE)
- `fig1b_mfd.pdf` — Figure 1(b) (MFD, mean + SE)
- `figA1a_big2.pdf` — Figure A1(a) (Big Two, individual models + SD)
- `figA1b_mfd.pdf` — Figure A1(b) (MFD, individual models + SD)

### Figure 2 — Base-model log-probability comparison

```bash
python figures/logprob_model_comparison.py
```

Reads pre-generated per-model CSVs from `data/logprobs/`. Produces in `figures/output/`:
- `big2_logprob_rank_gemma_vs_llamma_pretrained.png` — Figure 2b
- `big2_logprob_rank_gemma_vs_llamma_instruct.png` — Figure 2a

### Figure 3 — MWLR lollipop plot (Freedom vs Love)

```bash
python figures/generate_figure_3.py
```

Reads per-model CSVs from `data/logprobs/`. Produces in `figures/output/`:
- `fig3_lollipop.pdf` — Lollipop plot of MWLR scores across Llama × Gemma 2 model pairs

### Figure 4 — RM training checkpoints and ablation

```bash
Rscript figures/generate_figure_4.R
```

Requires the parquet checkpoint data in `data/reward_model_training/`. Produces in `figures/output/`:
- `fig4a_training_checkpoints.pdf` — Figure 4(a) (median rank over training steps)
- `fig4b_ablations.pdf` — Figure 4(b) (data source and quantity ablations)

## Data Provenance

- **Reward model scores** (`data/reward_model_scores/`): Generated by scoring dictionary tokens with 10 reward models from the [RewardBench](https://huggingface.co/spaces/allenai/reward-bench) leaderboard, spanning Gemma-based and Llama-based architectures (Skywork-Reward, ArmoRM, GRM, QRM, URM).
- **Value dictionaries** (`data/corpora/`): The Big Two (Agency/Communion) dictionary is from Abele et al.; the Moral Foundations Dictionary 2.0 is from Frimer et al.
- **Training data** (`data/reward_model_training/`): Checkpoint-level reward rankings from in-house training runs using Bradley–Terry and generative reward modeling objectives on UltraFeedback, Skywork, and GRM preference datasets at varying scales (13K–550K pairs).
- **Log-probabilities** (`data/logprobs/`): Per-model CSVs containing next-token log-probabilities (log-softmax) across the full vocabulary for all 54 prompts. Generated by `scripts/generate_logprobs.py` from 12 models: 2 pretrained (Gemma 2 2B, Llama 3.2 3B) and 10 instruction-tuned (3 Gemma 2 and 7 Llama 3–3.3, 1B–70B). CSVs are not committed to the repository; run the generation script to produce them.
