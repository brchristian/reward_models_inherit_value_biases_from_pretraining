# Reward Models Inherit Value Biases from Pretraining

This repository contains code and data for analyzing how reward models trained on different base models (Gemma vs. Llama) inherit value-related biases from their pretrained representations. We evaluate this using two established psychological instruments — the Big Two personality framework (Agency/Communion) and the Moral Foundations Dictionary 2.0 — by examining how 10 off-the-shelf reward models from RewardBench rank value-laden tokens, by tracing these these biases back to their pretrained base-models and the "implicit" RMs between them, and by tracking how these biases emerge during reward model training.

## Repository Structure

```
data/
├── corpora/                    Value dictionaries and token lists
│   ├── dict_big2_expanded.csv          Big Two dictionary (expanded)
│   ├── dict_big2_nouns.csv             Big Two dictionary (nouns only)
│   ├── moral-foundations-dictionary-20.dicx   MFD 2.0
│   └── token_intersection.csv          Shared tokens across model vocabularies
├── reward_model_scores/        Reward scores from 10 off-the-shelf models
│   ├── dict_big2_nouns/                Scores on Big Two nouns
│   └── dict_MFD_20/                    Scores on MFD 2.0 terms
└── reward_model_training/      Checkpoint-level data from in-house training runs
    ├── greatest_ever_one_across_checkpoints_gemma.parquet
    ├── greatest_ever_one_across_checkpoints_llama.parquet
    └── rank_results_*.csv              Ablation results at various data sizes

figures/
├── generate_figure_1_and_A1.R         Off-the-shelf RM comparison (Big Two & MFD)
├── generate_figure_4.R         Training checkpoint & ablation analysis
├── logprob_model_comparison.py Base-model log-probability comparison
└── output/                     Generated PDFs and PNGs
```

## Requirements

### R

Used by `generate_figure_1_and_A1.R` and `generate_figure_4.R`.

```r
install.packages(c(
  "tidyverse", "tidytext", "ggpubr", "ggbeeswarm",
  "extrafont", "nlme", "MKinfer",
  "jsonlite", "fuzzyjoin", "arrow"
))
```

### Python

Used by `logprob_model_comparison.py`. Requires a CUDA-capable GPU.

```
numpy
pandas
torch
transformers
huggingface_hub
seaborn
matplotlib
statsmodels
```

## Reproducing Figures

All scripts assume the working directory is the repository root.

### Figure 1 — Off-the-shelf reward model rankings (Big Two & MFD)

```bash
Rscript figures/generate_figure_1_and_A1.R
```

Produces in `figures/output/`:
- `fig1a_big2.pdf` — Figure 1(a) (Big Two, mean + SE)
- `fig1b_mfd.pdf` — Figure 1(b) (MFD, mean + SE)
- `figA1a_big2.pdf` — Figure A1(a) (Big Two, individual models + SD)
- `figA1b_mfd.pdf` — Figure A1(b) (MFD, individual models + SD)

### Figure 2 — Base-model log-probability comparison

```bash
python figures/logprob_model_comparison.py
```

Requires GPU and HuggingFace access to download Gemma 2 2B and Llama 3.2 3B. Produces in `figures/output/`:
- `big2_logprob_rank_gemma_vs_llamma_pretrained.png` — Figure 2b
- `big2_logprob_rank_gemma_vs_llamma_instruct.png` — Figure 2a

### Figure 4 — RM training checkpoints and ablation

```bash
Rscript figures/generate_figure_4.R
```

Requires the parquet checkpoint data in `data/reward_model_training/`. Produces in `figures/output/`:
- `fig4a_training_checkpoints.pdf` — Figure 4(a) (median rank over training steps)
- `fig4b_ablations.pdf` — Figure 4(b) (data source and quantity ablations)

## Data Provenance

- **Reward model scores** (`data/reward_model_scores/`): Generated by scoring dictionary tokens with 10 reward models from the [RewardBench](https://huggingface.co/spaces/allenai/reward-bench) leaderboard, spanning Gemma-based and Llama-based architectures (Skywork-Reward, ArmoRM, GRM, QRM, URM).
- **Value dictionaries** (`data/corpora/`): The Big Two (Agency/Communion) dictionary is from Abele et al.; the Moral Foundations Dictionary 2.0 is from Frimer et al.
- **Training data** (`data/reward_model_training/`): Checkpoint-level reward rankings from in-house training runs using Bradley–Terry and generative reward modeling objectives on UltraFeedback, Skywork, and GRM preference datasets at varying scales (13K–550K pairs).
